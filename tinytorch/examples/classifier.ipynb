{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network Classifier with TinyTorch üî•\n",
    "\n",
    "This notebook demonstrates how to build and train a neural network for binary classification using TinyTorch. We'll walk through each step of the machine learning pipeline, from data preparation to model evaluation, while explaining the key concepts along the way.\n",
    "\n",
    "## What We'll Cover\n",
    "1. Data preparation and visualization\n",
    "2. Building a multi-layer perceptron (MLP)\n",
    "3. Training the model with gradient descent\n",
    "4. Evaluating model performance\n",
    "5. Visualizing the decision boundary\n",
    "\n",
    "Let's start by installing the required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages in the current environment\n",
    "!uv pip install matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation üìä\n",
    "\n",
    "We'll use the `make_moons` dataset from scikit-learn, which creates two interleaving half circles. This is a classic binary classification problem that requires a nonlinear decision boundary, making it perfect for demonstrating the power of neural networks.\n",
    "\n",
    "The dataset has two features (x and y coordinates) and a binary label (0 or 1) for each point. We'll generate 1000 samples with some added noise to make the problem more realistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Generate the dataset\n",
    "data = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X = 4.0 * (data[0] - np.array([0.5, 0.25]))  # Scale and center the data\n",
    "y = data[1]\n",
    "\n",
    "# Visualize the dataset\n",
    "colors = np.array([\"#3057D3\", \"#D33030\"])  # Blue for class 0, Red for class 1\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y + 1) // 2])\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Two Moons Dataset\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/classification_data.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Before training our neural network, we need to:\n",
    "1. Split the data into training and test sets\n",
    "2. Standardize the features\n",
    "\n",
    "Standardization (scaling features to zero mean and unit variance) is crucial for neural networks as it:\n",
    "- Ensures all features contribute equally to the model\n",
    "- Helps with gradient descent convergence\n",
    "- Makes the training process more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split dataset into 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape} samples with {X_train.shape[1]} features\")\n",
    "print(f\"Test set shape:     {X_test.shape} samples with {X_test.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to TinyTorch Tensors\n",
    "\n",
    "Now we'll convert our NumPy arrays to TinyTorch tensors. TinyTorch tensors are the fundamental data structure that supports automatic differentiation, allowing us to compute gradients for training our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinytorch import Tensor\n",
    "\n",
    "# Create tinytorch tensor objects\n",
    "Xt_train = Tensor(X_train_scaled)\n",
    "yt_train = Tensor(y_train)\n",
    "Xt_test = Tensor(X_test_scaled)\n",
    "yt_test = Tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building the Neural Network üß†\n",
    "\n",
    "We'll create a Multi-Layer Perceptron (MLP) with the following architecture:\n",
    "- Input layer: 2 features\n",
    "- First hidden layer: 24 neurons with ReLU activation\n",
    "- Second hidden layer: 12 neurons with ReLU activation\n",
    "- Output layer: 1 neuron with Sigmoid activation\n",
    "\n",
    "This architecture was chosen because:\n",
    "1. The hidden layers with ReLU activation can learn complex nonlinear patterns\n",
    "2. The decreasing layer sizes (24 ‚Üí 12 ‚Üí 1) create a bottleneck that helps prevent overfitting\n",
    "3. The sigmoid output activation squashes values to [0,1], perfect for binary classification\n",
    "\n",
    "Let's create and inspect our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinytorch import MLP, Activation\n",
    "\n",
    "# Define the neural network architecture\n",
    "mlp = MLP(\n",
    "    n_input=2,  # Two input features\n",
    "    layers=[\n",
    "        (24, Activation.RELU),   # Hidden layer 1: 24 neurons with ReLU\n",
    "        (12, Activation.RELU),   # Hidden layer 2: 12 neurons with ReLU\n",
    "        (1, Activation.SIGMOID), # Output layer: 1 neuron with Sigmoid\n",
    "    ],\n",
    ")\n",
    "display(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Model üìà\n",
    "\n",
    "We'll train our model using:\n",
    "- Binary cross-entropy loss (appropriate for binary classification)\n",
    "- Gradient descent optimization\n",
    "- 200 epochs of training\n",
    "- Learning rate of 0.5\n",
    "\n",
    "During training, we'll track both loss and accuracy on training and test sets to monitor:\n",
    "1. How well the model is learning (training metrics)\n",
    "2. How well it generalizes (test metrics)\n",
    "3. Whether we're overfitting (gap between training and test performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "lr = 5e-1\n",
    "\n",
    "# Track both loss and accuracy\n",
    "metrics = {\"epoch\": [], \"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "# Training loop\n",
    "for i in range(0, epochs):\n",
    "    # Forward pass to get probabilities\n",
    "    y_train_probs = mlp(Xt_train)\n",
    "    y_test_probs = mlp(Xt_test)\n",
    "\n",
    "    # Calculate accuracy (probabilities > 0.5 for binary classification)\n",
    "    y_train_pred = (y_train_probs.data > 0.5).astype(np.float32)\n",
    "    y_test_pred = (y_test_probs.data > 0.5).astype(np.float32)\n",
    "\n",
    "    train_acc = np.mean(y_train_pred == yt_train.data)\n",
    "    test_acc = np.mean(y_test_pred == yt_test.data)\n",
    "\n",
    "    # Zero gradients before backward pass\n",
    "    mlp.flush_grads()\n",
    "\n",
    "    # Binary cross-entropy loss\n",
    "    neg_logl_train = -(\n",
    "        yt_train * y_train_probs.log() + (1 - yt_train) * (1 - y_train_probs).log()\n",
    "    ).sum() / len(yt_train)\n",
    "    neg_logl_test = -(\n",
    "        yt_test * y_test_probs.log() + (1 - yt_test) * (1 - y_test_probs).log()\n",
    "    ).sum() / len(yt_test)\n",
    "\n",
    "    # Store metrics for plotting\n",
    "    epoch = i + 1\n",
    "    train_loss = neg_logl_train.data.item()\n",
    "    test_loss = neg_logl_test.data.item()\n",
    "\n",
    "    print(\n",
    "        f\"epoch {epoch:03d}: \"\n",
    "        f\"loss[train]={train_loss:.3f}, loss[test]={test_loss:.3f} | \"\n",
    "        f\"acc[train]={train_acc:.3f}, acc[test]={test_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    metrics[\"epoch\"].append(epoch)\n",
    "    metrics[\"train_loss\"].append(train_loss)\n",
    "    metrics[\"test_loss\"].append(test_loss)\n",
    "    metrics[\"train_acc\"].append(train_acc)\n",
    "    metrics[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Backward pass to compute gradients\n",
    "    neg_logl_train.backward()\n",
    "\n",
    "    # Update parameters using gradient descent\n",
    "    for param in mlp.parameters:\n",
    "        param.data += -lr * param.grad\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "# Plot loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"train_loss\"], label=\"Train\")\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"test_loss\"], label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "\n",
    "# Plot accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"train_acc\"], label=\"Train\")\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"test_acc\"], label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs Epoch\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/classification_training.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing the Decision Boundary üé®\n",
    "\n",
    "To understand how our model separates the two classes, we'll visualize its decision boundary. We'll:\n",
    "1. Create a fine grid of points covering our feature space\n",
    "2. Get model predictions for each point\n",
    "3. Plot the probability contours and decision boundary\n",
    "\n",
    "This visualization helps us see:\n",
    "- The nonlinear nature of the learned decision boundary\n",
    "- Areas where the model is confident vs. uncertain\n",
    "- How well the boundary separates the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Create a fine mesh grid\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n",
    "X_mesh = np.c_[xx.ravel(), yy.ravel()]  # (150*150, 2)\n",
    "\n",
    "# Scale mesh points using the same scaler used for training data\n",
    "X_mesh_scaled = scaler.transform(X_mesh)\n",
    "\n",
    "# Get model predictions for mesh points\n",
    "X_mesh_tensor = Tensor(X_mesh_scaled)\n",
    "mesh_probs = mlp(X_mesh_tensor)\n",
    "mesh_probs = mesh_probs.data.reshape(xx.shape)\n",
    "\n",
    "# Create the visualization\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot probability contours\n",
    "norm = Normalize(vmin=0, vmax=1)\n",
    "levels = np.linspace(0, 1, num=11, endpoint=True)\n",
    "contour = plt.contourf(xx, yy, mesh_probs, alpha=0.3, cmap=\"RdBu_r\", norm=norm, levels=levels)\n",
    "plt.colorbar(contour, label=\"Probability of Class 1\")\n",
    "\n",
    "# Plot training points\n",
    "class_colors = [\"#3057D3\", \"#D33030\"]  # Blue for class 0, Red for class 1\n",
    "plt.scatter(\n",
    "    X_train[:, 0],\n",
    "    X_train[:, 1],\n",
    "    c=[class_colors[int(y)] for y in y_train],\n",
    "    s=10,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add decision boundary (where probability = 0.5)\n",
    "plt.contour(xx, yy, mesh_probs, levels=[0.5], colors=\"black\", linestyles=\"--\", linewidths=2)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Neural Network Decision Boundary\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/classification_results.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary üìù\n",
    "\n",
    "In this notebook, we've demonstrated how to:\n",
    "1. Prepare and preprocess data for neural network training\n",
    "2. Build a multi-layer perceptron using TinyTorch\n",
    "3. Train the model using gradient descent and binary cross-entropy loss\n",
    "4. Monitor training progress and evaluate model performance\n",
    "5. Visualize the learned decision boundary\n",
    "\n",
    "The resulting model successfully learns a nonlinear decision boundary that separates the two classes of our moon-shaped dataset. This example showcases the power of neural networks to learn complex patterns in data, all implemented using our minimal TinyTorch framework!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
