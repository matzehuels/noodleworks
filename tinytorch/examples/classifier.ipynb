{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Define dataset\n",
    "data = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X = 4.0 * (data[0] - np.array([0.5, 0.25]))\n",
    "y = data[1]\n",
    "\n",
    "# Plot dataset\n",
    "colors = np.array([\"#3057D3\", \"#D33030\"])\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[(y + 1) // 2])\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Data\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/classification_data.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"y_train.shape: {y_train.shape}\")\n",
    "print(f\" X_test.shape: {X_test.shape}\")\n",
    "print(f\" y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinytorch import Tensor\n",
    "\n",
    "# Create tinytorch tensor objects\n",
    "Xt_train = Tensor(X_train_scaled)\n",
    "yt_train = Tensor(y_train)\n",
    "Xt_test = Tensor(X_test_scaled)\n",
    "yt_test = Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinytorch import MLP, Activation, Tensor\n",
    "\n",
    "# Define a simple mlp for classification\n",
    "mlp = MLP(\n",
    "    n_input=2,\n",
    "    layers=[\n",
    "        (24, Activation.RELU),  # hidden layer 1\n",
    "        (12, Activation.RELU),  # hidden layer 2\n",
    "        (1, Activation.SIGMOID),  # output layer: 1 neuron with Sigmoid activation\n",
    "    ],\n",
    ")\n",
    "display(mlp)\n",
    "\n",
    "epochs = 200\n",
    "lr = 5e-1\n",
    "\n",
    "# Track both loss and accuracy\n",
    "metrics = {\"epoch\": [], \"train_loss\": [], \"test_loss\": [], \"train_acc\": [], \"test_acc\": []}\n",
    "\n",
    "# Training loop\n",
    "for i in range(0, epochs):\n",
    "    # Forward pass to get probabilities\n",
    "    y_train_probs = mlp(Xt_train)\n",
    "    y_test_probs = mlp(Xt_test)\n",
    "\n",
    "    # Calculate accuracy (probabilities > 0.5 for binary classification)\n",
    "    y_train_pred = (y_train_probs.data > 0.5).astype(np.float32)\n",
    "    y_test_pred = (y_test_probs.data > 0.5).astype(np.float32)\n",
    "\n",
    "    train_acc = np.mean(y_train_pred == yt_train.data)\n",
    "    test_acc = np.mean(y_test_pred == yt_test.data)\n",
    "\n",
    "    # Zero gradients\n",
    "    mlp.flush_grads()\n",
    "\n",
    "    # Binary cross-entropy loss\n",
    "    neg_logl_train = -(\n",
    "        yt_train * y_train_probs.log() + (1 - yt_train) * (1 - y_train_probs).log()\n",
    "    ).sum() / len(yt_train)\n",
    "    neg_logl_test = -(\n",
    "        yt_test * y_test_probs.log() + (1 - yt_test) * (1 - y_test_probs).log()\n",
    "    ).sum() / len(yt_test)\n",
    "\n",
    "    # Bookkeeping\n",
    "    epoch = i + 1\n",
    "    train_loss = neg_logl_train.data.item()\n",
    "    test_loss = neg_logl_test.data.item()\n",
    "\n",
    "    print(\n",
    "        f\"epoch {epoch:03d}: \"\n",
    "        f\"loss[train]={train_loss:.3f}, loss[test]={test_loss:.3f} | \"\n",
    "        f\"acc[train]={train_acc:.3f}, acc[test]={test_acc:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Store metrics\n",
    "    metrics[\"epoch\"].append(epoch)\n",
    "    metrics[\"train_loss\"].append(train_loss)\n",
    "    metrics[\"test_loss\"].append(test_loss)\n",
    "    metrics[\"train_acc\"].append(train_acc)\n",
    "    metrics[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # Backward pass\n",
    "    neg_logl_train.backward()\n",
    "\n",
    "    # Gradient descent update to parameters\n",
    "    for param in mlp.parameters:\n",
    "        param.data += -lr * param.grad\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(8, 3))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"train_loss\"], label=\"Train\")\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"test_loss\"], label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Loss vs Epoch\")\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"train_acc\"], label=\"Train\")\n",
    "plt.plot(metrics[\"epoch\"], metrics[\"test_acc\"], label=\"Test\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs Epoch\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/classification_training.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Create meshgrid for visualization\n",
    "xx, yy = np.meshgrid(np.linspace(-7, 7, 150), np.linspace(-7, 7, 150))\n",
    "X_mesh = np.c_[xx.ravel(), yy.ravel()]  # (150*150, 2)\n",
    "\n",
    "# Scale mesh points using the same scaler used for training data\n",
    "X_mesh_scaled = scaler.transform(X_mesh)\n",
    "\n",
    "# Get predictions for mesh points\n",
    "X_mesh_tensor = Tensor(X_mesh_scaled)\n",
    "mesh_probs = mlp(X_mesh_tensor)\n",
    "mesh_probs = mesh_probs.data.reshape(xx.shape)\n",
    "\n",
    "# Plot decision boundary and data points\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "# Plot decision boundary with red-blue gradient\n",
    "norm = Normalize(vmin=0, vmax=1)\n",
    "levels = np.linspace(0, 1, num=11, endpoint=True)\n",
    "contour = plt.contourf(xx, yy, mesh_probs, alpha=0.3, cmap=\"RdBu_r\", norm=norm, levels=levels)\n",
    "plt.colorbar(contour, label=\"Probability\")\n",
    "\n",
    "# Plot data points with matching red/blue colors\n",
    "class_colors = [\"#3057D3\", \"#D33030\"]  # Blue, Red\n",
    "plt.scatter(\n",
    "    X_train[:, 0],\n",
    "    X_train[:, 1],\n",
    "    c=[class_colors[int(y)] for y in y_train],\n",
    "    s=10,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "# Add decision boundary contour\n",
    "plt.contour(xx, yy, mesh_probs, levels=[0.5], colors=\"black\", linestyles=\"--\", linewidths=2)\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../assets/classification_results.svg\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}